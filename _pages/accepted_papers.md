---
layout: page
permalink: /accepted_papers/
title: Poster Sessions
description:
nav: true
nav_order: 4
---

<h1>Poster Session 1</h1>

- Is Value Learning Really the Main Bottleneck in Offline RL? <br>
- REBEL: Reinforcement Learning via Regressing Relative Rewards <br>
- Partially Observable Multi-Agent Reinforcement Learning using Mean Field Control <br>
- Information Theoretic Guarantees For Policy Alignment In Large Language Models <br>
- An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models <br>
- Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent <br>
- When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL <br>
- Transferable Reinforcement Learning via Generalized Occupancy Models <br>
- Learning to Steer Markovian Agents under Model Uncertainty <br>
- vMF-exp: von Mises-Fisher Exploration of Large Action Sets with Hyperspherical Embeddings <br>
- No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO <br>
- Adaptive Foundation Models for Online Decisions: HyperAgent with Fast Incremental Uncertainty Estimation <br>
- Efficient Offline Learning of Ranking Policies via Top-$k$ Policy Decomposition <br>
- Oracle-Efficient Reinforcement Learning for Max Value Ensembles <br>
- Policy Gradient Methods with Adaptive Policy Spaces <br>
- Functional Acceleration for Policy Mirror Descent <br>
- VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation <br>
- Multi-Agent Imitation Learning: Value is Easy, Regret is Hard <br>
- Batch Learning via Log-Sum-Exponential Estimator from Logged Bandit Feedback <br>
- Advantage Alignment Algorithms <br>
- Enhancing Actor-Critic Decision-Making with Afterstate Models for Continuous Control <br>
- In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning <br>
- Reinforcement Learning from Bagged Reward <br>
- Combining Reconstruction and Contrastive Methods for Multimodal Representations in RL <br>
- Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning <br>
- Safe exploration in reproducing kernel Hilbert spaces <br>
- PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling <br>
- Towards Zero-Shot Generalization in Offline Reinforcement Learning <br>
- Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control <br>
- On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics <br>
- BenchMARL: Benchmarking Multi-Agent Reinforcement Learning <br>
- Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts <br>
- Adaptive Two-Level Quasi-Monte Carlo for Soft Actor-Critic <br>
- Markov Persuasion Processes: How to Persuade Multiple Agents From Scratch <br>
- Delayed Adversarial Attacks on Stochastic Multi-Armed Bandits <br>
- RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation <br>
- Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies <br>
- Idea Track: Leveraging Reinforcement Learning to Enhance Decision-Making in Oncology Treatments <br>
- Idea Track: Tight Bounds for Bernoulli Rewards in Kernelized Multi-Armed Bandits <br>
- Idea Track: Reward Estimation in Inverse Bandit Problems <br>
- Idea Track: Proper Hyper-parameter Optimization in Reinforcement Learning <br>

<br><br>
<h1>Poster Session 2</h1>

- A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits <br>
- Transductive Active Learning with Application to Safe Bayesian Optimization <br>
- Realtime Reinforcement Learning: Towards Rapid Asynchronous Deployment of Large Models <br>
- Contextualized Hybrid Ensemble Q-learning: Learning Fast with Control Priors <br>
- Improved Algorithms for Adversarial Bandits with Unbounded Losses <br>
- Survive on Planet Pandora: Robust Cross-Domain RL Under Distinct State-Action Representations <br>
- A Case for Validation Buffer in Pessimistic Actor-Critic <br>
- Offline RL via Feature-Occupancy Gradient Ascent <br>
- How Does Return Distribution in Distributional Reinforcement Learning Help Optimization? <br>
- Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning <br>
- Exploiting Exogenous Structure for Sample-Efficient Reinforcement Learning <br>
- Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity <br>
- A Theoretical Framework for Partially-Observed Reward States in RLHF <br>
- Reinforcement Learning in the Wild with Maximum Likelihood-based Model Transfer <br>
- Dual Approximation Policy Optimization <br>
- Wind farm control with cooperative multi-agent reinforcement learning <br>
- Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm <br>
- Provable Partially Observable Reinforcement Learning with Privileged Information <br>
- Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer <br>
- KalMamba: Towards Efficient Probabilistic State Space Models for RL under Uncertainty <br>
- Batched fixed-confidence pure exploration for bandits with switching constraints <br>
- Offline Reinforcement Learning with Pessimistic Value Priors <br>
- The Importance of Online Data: Understanding Preference Fine-Tuning via Coverage <br>
- Generalized Linear Bandits with Limited Adaptivity <br>
- Coordination Failure in Cooperative Offline MARL <br>
- Efficient Offline Reinforcement Learning: The Critic is Critical <br>
- EMPO: A Clustering-Based On-Policy Algorithm for Offline Reinforcement Learing <br>
- Quantized Representations Prevent Dimensional Collapse in Self-predictive RL <br>
- A Tractable Inference Perspective of Offline RL <br>
- Risk-Aware Bandits for Best Crop Management <br>
- Decoupled Stochastic Gradient Descent for N-Player Games <br>
- ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization <br>
- Misspecified $Q$-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error <br>
- Handling Delay in Reinforcement Learning Caused by Parallel Computations of Neurons <br>
- Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions <br>
- Reweighted Bellman Targets for Continual Reinforcement Learning <br>
- Should You Trust DQN? <br>
- Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently <br>
- Idea Track: Improving Sample Efficiency in World Models through Semantic Exploration via Expert Demonstration <br>
- Idea Track: Active Representation Learning <br>
- Idea Track: Better Gradient Steps for Deep On-Policy Reinforcement Learning <br>
